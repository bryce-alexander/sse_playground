{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Case Study - Method Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Candidate Assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from assistant_helpers import create_assistant, prompt_assistant, evaluate_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1 - Naive Assistant\n",
    "No instruction optimization, no prompt optimization, no temperature optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_assistant = create_assistant()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2 - Simple Prompt\n",
    "No adjustment to instructions.  Uses simple prompt w/ lowered temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_prompt_assistant = create_assistant(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 3 - Rules-based Instructions\n",
    "Uses more explicit rules-based instructions and lowered temperature.  Simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_based_assistant = create_assistant(instructions='You are an assistant that MUST provide complete and untruncated answers. You MUST strictly follow these non-negotiable rules:\\n- You MUST prioritize completeness in your response over being concise.\\n- When reviewing information in a CSV, you can ONLY base your answer on the full and untruncated content of each field.\\n- When processing CSV content, you MUST convert CSV content to a dictionary and ONLY base your answer on the dictionary output.\\n- You MUST strictly follow all of these rules.\\n-You will NEVER use data frame outputs to answer your question in Code Interpreter.\\n\\nIf you cannot fully comply with these rules, you MUST explicitly state why.', temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 4 - Step-based Instructions\n",
    "Uses more explicit step-based instructions and lowered temperature. Simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_based_assistant = create_assistant(instructions='You are an assistant that provides full and untruncated responses to questions.  The user will provide a CSV and you will take the following steps to arrive at your answer:\\nStep 1 - Convert the CSV to a dictionary.\\nStep 2 - Review the FULL contents of the dictionary to understand the context required to answer the question.\\nStep 3 - Using ONLY dictionary outputs, answer the user question COMPLETELY and without leaving out ANY details.\\n\\nFollowing these steps EXACTLY is STRICTLY REQUIRED and NON-NEGOTIABLE.  You MUST NOT use dataframe output in your response.', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 5 - Function Calls\n",
    "Uses a function call to force CSV to be processed as JSON; no adjustment to temperature.  Simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_call_assistant = create_assistant(instructions='You are an assistant tasked with providing complete and untruncated answers to questions.  You must prioritize completeness over being concise.  ALL uploaded CSVs need to be converted by the function \\'process_csv\\' and will be returned as a JSON formatted string.  ALWAYS parse the JSON string before leveraging data to ask a question.', enable_function=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluations\n",
    "Run each method through a series of 20 trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0/20 complete.\n",
      "Run 1/20 complete.\n",
      "Run 2/20 complete.\n",
      "Run 3/20 complete.\n",
      "Run 4/20 complete.\n",
      "Run 5/20 complete.\n",
      "Run 6/20 complete.\n",
      "Run 7/20 complete.\n",
      "Run 8/20 complete.\n",
      "Run 9/20 complete.\n",
      "Run 10/20 complete.\n",
      "Run 11/20 complete.\n",
      "Run 12/20 complete.\n",
      "Run 13/20 complete.\n",
      "Run 14/20 complete.\n",
      "Run 15/20 complete.\n",
      "Run 16/20 complete.\n",
      "Run 17/20 complete.\n",
      "Run 18/20 complete.\n",
      "Run 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Evaluation 19/20 complete.\n",
      "Run 0/20 complete.\n",
      "Run 1/20 complete.\n"
     ]
    }
   ],
   "source": [
    "# Configure assistants and prompts\n",
    "import pandas as pd\n",
    "\n",
    "experiments = [\n",
    "    [naive_assistant, 'Tell me tina escobars favorite city and why'],\n",
    "    [simple_prompt_assistant, 'Review the attached CSV and extract all information from all fields.  DO NOT use dataframe output to get your answer. Tell me Tina Escobars favorite city and why, including ALL details in the relevant field.'],\n",
    "    [rules_based_assistant, 'Tell me what tina escobars favorite city is and why'],\n",
    "    [step_based_assistant, 'Tell me what tina escobars favorite city is and why'],\n",
    "    [function_call_assistant, 'Tell me what tina escobars favorite city is and why']\n",
    "    ]\n",
    "\n",
    "# Run experiments and get results\n",
    "results = []\n",
    "averages = []\n",
    "for experiment in experiments:\n",
    "    assistant_, prompt_ = experiment\n",
    "\n",
    "    result = evaluate_method(assistant=assistant_, prompt=prompt_, file_path='tse_takehome_dataset.csv', debug=False, runs=20)\n",
    "\n",
    "    results.append(result)\n",
    "    averages.append(pd.to_numeric(result[1]['Accurate']).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create labels\n",
    "labels = ['Naive','Simple Prompt','Rules-Based','Step-Based','Function Call']\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, averages)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Method\")\n",
    "plt.ylabel(\"Average Accuracy\")\n",
    "plt.title(\"Method vs Avg Accuracy\")\n",
    "\n",
    "# Show the plot\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
